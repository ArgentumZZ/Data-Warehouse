## Data Warehouse

- **Data Warehouse**
  - Extract, transform and load data from different sources into a PostgresSQL database.
- **Main folders**
  - `dags` - DAG files for Airflow.
  - `connectors` - Connectors to different DB and non-DB sources.
  - `utilities` - Utilities files.
  - `metadata/logs` - Logs generated for each project's run.
  - `metadata/output` - Files generated for each project's run.
  - `tests` - Unit tests.
  - `warehouse` - Dim and fact tables.
  - `views` - Custom views.
  - `data_quality_checks` - Custom data quality checks.
  - `aggregations` - Data aggregations.
  - `docker` - Dockerfile, requirements.txt and .sh run files.
  - `customer_code` - Custom code for each project.
  - `sript_factory` - Central assembly factory, take info from all other files to create the tasks for execution.
  - `script_runner` - Files (`.bat / .sh`) that run `script_runner.py` which initializes the `script_factory.py`.
- **Main files**
  - `etl_audit_manager.py` - audit table.
  - `etl_utils.py` - general ETL utils functions.
  - `script_worker.py` - custom functions for a given project. 
  - `sql_queries.py` - SQL queries that are parametrized.
  - `alter_tables.sql` - Track executed SQL queries. 
  - `script_factory.py` - Assemble the tasks for the project.
  - `script_runner.py` - Run the tasks in `script_factory.py`.
  - `.bat/.sh` files to run `script_runner.py`.
-------------------------------
## General structure
```
datawarehouse/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ venv/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ local/
â”‚   â”‚   â”œâ”€â”€ db_config.cfg
â”‚   â”‚   â”œâ”€â”€ keyfile_1.pem
â”‚   â”‚   â”œâ”€â”€ keyfile_2.pkk
â”‚   â”‚   â”œâ”€â”€ setenv.bat
â”‚   â”‚   â”œâ”€â”€ setenv.sh
â”‚   â”‚   â””â”€â”€ ... other pem/pkk files ...
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dwh_main_dag.py
â”‚   â””â”€â”€ ... other DAG .py files ...
â”‚
â”œâ”€â”€ etls/
â”‚   â”œâ”€â”€ _templates/
â”‚   â”‚   â””â”€â”€ (general template files)
â”‚   â”‚
â”‚   â”œâ”€â”€ aggregations/
â”‚   â”‚   â”œâ”€â”€ aggregation_1_revenue/
â”‚   â”‚   â””â”€â”€ ... other aggregation projects ...
â”‚   â”‚
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ postgresql_connector.py
â”‚   â”‚   â”œâ”€â”€ mysql_connector.py
â”‚   â”‚   â”œâ”€â”€ oracle_connector.py
â”‚   â”‚   â””â”€â”€ ... other connectors...
â”‚   â”‚
â”‚   â”œâ”€â”€ data_quality_checks/
â”‚   â”‚   â”œâ”€â”€ dqc_1_calculate_record_discrepancies/
â”‚   â”‚   â””â”€â”€ ... other DQC projects ...
â”‚   â”‚
â”‚   â”œâ”€â”€ datastore/
â”‚   â”‚   â”œâ”€â”€ alpaca_1_revenue/
â”‚   â”‚   â”œâ”€â”€ crypto_1_transactions/
â”‚   â”‚   â”œâ”€â”€ financial_data_1_ethereum/
â”‚   â”‚   â””â”€â”€ ... other project folders ...
â”‚   â”‚
â”‚   â”œâ”€â”€ utilities/
â”‚   â”‚   â”œâ”€â”€ argument_parser.py
â”‚   â”‚   â”œâ”€â”€ email_manager.py
â”‚   â”‚   â”œâ”€â”€ etl_audit_manager.py
â”‚   â”‚   â”œâ”€â”€ etl_utils.py
â”‚   â”‚   â”œâ”€â”€ date_utils.py
â”‚   â”‚   â”œâ”€â”€ db_utils.py
â”‚   â”‚   â”œâ”€â”€ dq_utils.py
â”‚   â”‚   â”œâ”€â”€ file_utils.py
â”‚   â”‚   â””â”€â”€ ... other utils files ...
â”‚   â”‚
â”‚   â””â”€â”€ warehouse/
â”‚       â”œâ”€â”€ dim_1_dim_crypto_transactions/
â”‚       â”œâ”€â”€ dim_1_staging_crypto_transactions/
â”‚       â”œâ”€â”€ fact_1_fact_shares_revenue/
â”‚       â”œâ”€â”€ fact_1_staging_shares_revenue/
â”‚       â””â”€â”€ views/
```
___
## ğŸ“ Project Toâ€‘Do Plan

- **Partial Task Functions**
  - Add parameterâ€‘accepting partial functions inside `script_factory.py` .âœ”ï¸
  - Improve modularity and reusability of task definitions. âœ”ï¸

- **ETL Audit Manager**
  - Create an audit table to track project's run metadata. âœ”ï¸
  - Capture `start/end_load_date`, `start/end_script_execution_time`,` data_min/max_dates`, `status`, `number_of_records`, `environment`, `script_version`, `load_type`, `previous_max_date`, `target_database`, `target_table`. âœ”ï¸ 
  - Create a `create_etl_runs_table` function. âœ”ï¸
  - Create an `insert_etl_runs_record` function. âœ”ï¸
  - Create an `update_etl_runs_record` function. âœ”ï¸
  
- **ETL Utilities**
  - Add custom ETL transformation functions. âœ”ï¸
  - Add a single `transform_dataframe` function that applies transformations. âœ”ï¸
  - Add `process_dataframe_date_ranges` function to calculate `data_min_date` and `data_max_date`. âœ”ï¸

- **Incremental and full Load**
  - Implement logic for both incremental (I) and full (F) load modes. âœ”ï¸
  - Override internal defaults with values from `.bat file.` âœ”ï¸

- **Logging**
  - Create a logging_manager.py to standardize log format. âœ”ï¸
  - Create a `logs/` folder to store logs for each project's run. âœ”ï¸
  - (Optionally) Automatically delete .logs older than 7 days. âœ”ï¸
  - (Optionally) Automatically delete .logs older than N runs. âœ”ï¸
  
- **Utilities folder**
  - Create utilities .py files for ETL processes (e.g. file_utils.py, argument_parser.py, errors_utils, db_utils.py)ï¸. âœ”ï¸

- **Output folder**
  - Create an `output/` folder to store generated files. âœ”ï¸
  - Format `output/file_name_timestamp.csv`. âœ”ï¸
  - Control with a boolean operator, whether the file will be deleted from the folder. âœ”ï¸

- **Containerization** (in progress)
  - Add a `Dockerfile` for containerized execution. âœ”ï¸
  - Update `_docker.bat` to run the container. âœ”ï¸
  - Update `_docker.sh` to run the container.
  - Ensure compatibility with Windows/Linux.

- **Launcher scripts** (in progress)
  - Update `.bat` âœ”ï¸, `_docker.bat` âœ”ï¸, `.sh`, `_docker.sh`.
  - Add parameter parsing and variable definitions. âœ”ï¸
  - Add logs and error handling. âœ”ï¸

- **Email Notifications (SMTP)**
  - Implement e-mail success/failure alerts after each project's run.
  - Include run summary and error details.

- **Backfill**
  - Implement backfill loading option in `.bat` âœ”ï¸, `_docker.bat` âœ”ï¸, `.sh`, `_docker.sh` files.
  - Create a separate backfill project that accepts `project_name`, `start_date`, `end_date`, `load_days` to run a given project and load data incrementally.

- **Connectors**
  - Add more connectors:
  - PostgreSQL (implemented) âœ”ï¸  
  - Oracle
  - MySQL
  - Salesforce
  - MSSQL
  - Snowflake
  - Google BigQuery
  - S3