## ğŸ›¢ï¸Data Warehouse

- **Data Warehouse**
  - A personal data warehouse ecosystem project that will implement modern dataâ€‘engineering patterns. 
  - The goal is to build a scalable, maintainable platform for analytics.
  - The project will include a modular ETL framework, Dockerized execution environment, Airflow orchestration, configurationâ€‘based execution. 
  - It will provide reusable connectors and utilities, a structured warehouse layer with dimensional and fact tables, views, email notifications, data quality checks, data warehouse process monitoring and backfilling for historical data recovery.
  
- **Main folders**
  - `dags` - DAG files for Airflow.
  - `connectors` - Connectors to different DB and non-DB sources.
  - `utilities` - Utilities files.
  - `metadata/logs` - Logs generated for each project's run.
  - `metadata/output` - Files generated for each project's run.
  - `tests` - Unit tests.
  - `warehouse` - Dimensional and fact tables.
  - `views` - Custom views.
  - `data_quality_checks` - Custom data quality checks (record discrepancies, stale projects, missing keys, null values, duplicates).
  - `aggregations` - Data aggregations - windowing (time-based), dimensional grouping (bucketing), change detection (delta aggregations), cumulative sum (running totals).
  - `docker` - Dockerfile, requirements.txt and .sh run files.
  - `custom_code` - Custom code (.py files) for each project.
  - `sript_factory` - A central assembly factory that builds the tasks for execution.
  - `script_runner` - Files (`.bat / .sh`) that run `run_script.py`.
- **Main files**
  - `etl_audit_manager.py` - An audit table that keeps track of project's run metadata.
  - `etl_utils.py` - ETL transformation functions.
  - `script_worker.py` - Custom functions for a given project. 
  - `sql_queries.py` - Parametrized SQL queries.
  - `alter_tables.sql` - History of executed SQL queries. 
  - `script_factory.py` - Assemble the tasks for the project.
  - `script_parameters.py` - Custom project parameters (script_name, version, load type, etc.)
  - `run_script.py` - Runs the tasks in `script_factory.py`.
  - `.bat/.sh` - Files used to run `run_script.py`.
  - `Dockerfile` + `_docker.bat`/`_docker.sh`  - Defines the docker metadata, builds an image and runs the container. 
-------------------------------
## ğŸ“ Folder structure
```
datawarehouse/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ venv/
â”œâ”€â”€ requirements_python_3_14.txt
â”‚
â”œâ”€â”€ orchestration
â”‚   â”œâ”€â”€ logs/
â”‚   â”‚     â”œâ”€â”€ dag_id = dwh_main_dag
â”‚   â”‚     â”‚     â”œâ”€â”€ run_id=manual__2026-01-27T09ï€º29ï€º54.691723+00ï€º00
â”‚   â”‚     â”‚     â””â”€â”€ other run_ids ...
â”‚   â”‚     â”œâ”€â”€ dag_processor_manager
â”‚   â”‚     â””â”€â”€ scheduler
â”‚   â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ docker_compose.yaml
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ local/
â”‚       â”œâ”€â”€ db_config.cfg
â”‚       â”œâ”€â”€ keyfile_1.pem
â”‚       â”œâ”€â”€ keyfile_2.pkk
â”‚       â”œâ”€â”€ api_credentials.json
â”‚       â”œâ”€â”€ setenv.bat
â”‚       â”œâ”€â”€ setenv.sh
â”‚       â””â”€â”€ ... other credential files ...
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ dwh_main_dag.py
â”‚   â””â”€â”€ ... other DAG .py files ...
â”‚
â”œâ”€â”€ etls/
â”‚   â”œâ”€â”€ _templates/
â”‚   â”‚   â””â”€â”€ (general template files)
â”‚   â”‚
â”‚   â”œâ”€â”€ aggregations/
â”‚   â”‚   â”œâ”€â”€ aggregation_1_revenue/
â”‚   â”‚   â””â”€â”€ ... other aggregation projects ...
â”‚   â”‚
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ postgresql_connector.py
â”‚   â”‚   â”œâ”€â”€ mysql_connector.py
â”‚   â”‚   â”œâ”€â”€ oracle_connector.py
â”‚   â”‚   â””â”€â”€ ... other connectors...
â”‚   â”‚
â”‚   â”œâ”€â”€ data_quality_checks/
â”‚   â”‚   â”œâ”€â”€ dqc_1_calculate_record_discrepancies/
â”‚   â”‚   â””â”€â”€ ... other DQC projects ...
â”‚   â”‚
â”‚   â”œâ”€â”€ datastore/
â”‚   â”‚   â”œâ”€â”€ financial_data_1_ethereum/
â”‚   â”‚   â”‚       â”œâ”€â”€ custom_code/
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ alter_tables.sql
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ script_factory.py
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ script_parameters.py
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ script_worker.py
â”‚   â”‚   â”‚       â”‚      â””â”€â”€ sql_queries.py
â”‚   â”‚   â”‚       â”œâ”€â”€ docker/
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ requirements.txt
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ run_financial_data_1_ethereum_docker.bat
â”‚   â”‚   â”‚       â”‚      â””â”€â”€ run_financial_data_1_ethereum_docker.sh
â”‚   â”‚   â”‚       â”œâ”€â”€ metadata/
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ logs/
â”‚   â”‚   â”‚       â”‚      â”‚      â”œâ”€â”€ 2026-01-18_11-36-19_etl.log
â”‚   â”‚   â”‚       â”‚      â”‚      â”œâ”€â”€ 2026-01-18_11-40-17_etl.log
â”‚   â”‚   â”‚       â”‚      â”‚      â””â”€â”€ ... other log _etl.log files ...
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ output/
â”‚   â”‚   â”‚       â”‚      â”‚      â”œâ”€â”€ 2026-01-18_11-36-19.csv
â”‚   â”‚   â”‚       â”‚      â”‚      â”œâ”€â”€ 2026-01-18_11-40-17.csv
â”‚   â”‚   â”‚       â”‚      â”‚      â””â”€â”€ ... other output .csv files ...
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ dictionary.yaml
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ mapping.yaml
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ schema.yaml
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ validation.yaml
â”‚   â”‚   â”‚       â”‚      â””â”€â”€ ... other .yaml files ...
â”‚   â”‚   â”‚       â”œâ”€â”€ script_runner/
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ run_financial_data_1_ethereum.bat
â”‚   â”‚   â”‚       â”‚      â”œâ”€â”€ run_financial_data_1_ethereum.sh
â”‚   â”‚   â”‚       â”‚      â””â”€â”€ run_script.py
â”‚   â”‚   â”‚       â””â”€â”€  __init__.py
â”‚   â”‚   â”œâ”€â”€ alpaca_1_revenue/
â”‚   â”‚   â”œâ”€â”€ crypto_1_transactions/
â”‚   â”‚   â””â”€â”€ ... other datastore project folders ...
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ test_connectors.py
â”‚   â”‚   â”œâ”€â”€ test_utils.py
â”‚   â”‚   â”œâ”€â”€ test_worker.py
â”‚   â”‚   â””â”€â”€ ... other test .py files ...
â”‚   â”‚
â”‚   â”œâ”€â”€ utilities/
â”‚   â”‚   â”œâ”€â”€ __init__.py 
â”‚   â”‚   â”œâ”€â”€ argument_parser.py
â”‚   â”‚   â”œâ”€â”€ email_manager.py
â”‚   â”‚   â”œâ”€â”€ etl_audit_manager.py
â”‚   â”‚   â”œâ”€â”€ etl_utils.py
â”‚   â”‚   â”œâ”€â”€ date_utils.py
â”‚   â”‚   â”œâ”€â”€ db_utils.py
â”‚   â”‚   â”œâ”€â”€ dq_utils.py
â”‚   â”‚   â”œâ”€â”€ file_utils.py
â”‚   â”‚   â””â”€â”€ ... other utils files ...
â”‚   â”‚
â”‚   â””â”€â”€ warehouse/
â”‚       â”œâ”€â”€ dim_1_dim_crypto_transactions/
â”‚       â”œâ”€â”€ dim_1_staging_crypto_transactions/
â”‚       â”œâ”€â”€ fact_1_fact_shares_revenue/
â”‚       â”œâ”€â”€ fact_1_staging_shares_revenue/
â”‚       â”œâ”€â”€ ... other warehouse project folders ...
â”‚       â”‚  
â”‚       â””â”€â”€ views/
â”‚             â”œâ”€â”€ view_1_revenue.py
â”‚             â””â”€â”€ ... other view files ...
```
___
## ğŸ“ Project Toâ€‘Do Plan

- **Partial task functions**
  - Add parameterâ€‘accepting partial functions inside `script_factory.py`. âœ”ï¸
  - Improve modularity and reusability of task definitions. âœ”ï¸
  - Add task name, description, retries, is_enabled and dependency parameters. âœ”ï¸
  - Add retry, enabled and dependency checks in `run_script.py`. âœ”ï¸

- **ETL audit manager**
  - Create an audit table to track project's run metadata. âœ”ï¸
  - Capture `start/end_load_date`, `start/end_script_execution_time`,` data_min/max_dates`, `status`, `number_of_records`, `environment`, `script_version`, `load_type`, `previous_max_date`, `target_database`, `target_table`. âœ”ï¸ 
  - Create a `create_etl_runs_table` function. âœ”ï¸
  - Create a `_calculate_etl_window` internal function to calculate sdt and edt. âœ”ï¸
  - Create an `insert_etl_runs_record` function. âœ”ï¸
  - Create an `update_etl_runs_record` function. âœ”ï¸
  
- **ETL utilities**
  - Add custom ETL transformation functions. âœ”ï¸
  - Add a single `transform_dataframe` function that applies transformations. âœ”ï¸
  - Add a `process_dataframe_date_ranges` function to calculate `data_min_date` and `data_max_date`. âœ”ï¸
  - Add a `check_source_date_range` function to determine whether the source data is up to date.
  - Add a `run_data_quality_check` function to run custom data quality checks.
  - Add a `delete_target_dates` function for partition-based deletion for fact tables.
  - Add a `set_reference_page` function to create a link to the corresponding ETL reference page in Confluence. 
  
- **Incremental and full Load**
  - Implement logic for incremental (I) and full (F) load modes. âœ”ï¸
  - Override internal defaults with values from `.bat file.` âœ”ï¸

- **Logging**
  - Create a logging_manager.py to standardize log format. âœ”ï¸
  - Create a `logs/` folder to store logs for each project's run. âœ”ï¸
  - Add a `cleanup_old_logs` function that automatically deletes .logs older than N days ot older than N runs. âœ”ï¸
  - Add a `get_current_log_content` function to read the current log. âœ”ï¸
  - Add a `get_current_log_size` function that acts as a pointer and returns the current log size. âœ”ï¸

- **Utilities folder**
  - `argument_parser.py` - Reads the arguments from .bat <param_1> <param_2> ... <param_n>. âœ”ï¸
  - `config_utils.py` - Reads the credentials in configuration files (e.g., _.cfg). âœ”ï¸
  - `db_utils.py` - Database utilities.
  - `dq_utils.py` - Data quality utilities.
  - `email_manager.py` - Create and send e-mails. âœ”ï¸
  - `error_utils.py` - Custom classes for error handling. âœ”ï¸
  - `file_utils.py` - File path and folder utility functions. âœ”ï¸
  - `etl_audit_manager.py` - Custom audit table. âœ”ï¸
  - `etl_utils.py` - Custom ETL transformations. âœ”ï¸
  - `logging_manager.py` - Custom logging handlers, formatters, traceback and stack level. âœ”ï¸

- **Output folder**
  - Create an `output/` folder to store generated files. âœ”ï¸
  - Store output in format `output/file_name_timestamp.csv`. âœ”ï¸
  - Add a boolean operator to control whether the file will be deleted from the folder. âœ”ï¸

- **Containerization**
  - Add a `Dockerfile` for containerized execution. âœ”ï¸
  - Update `_docker.bat` to run the container. âœ”ï¸
  - Update `_docker.sh` to run the container.
  - Ensure compatibility with Windows/Linux.

- **Launcher scripts**
  - Update `.bat` âœ”ï¸, `_docker.bat` âœ”ï¸, `.sh`, `_docker.sh`.
  - Add parameter parsing and variable definitions. âœ”ï¸
  - Add echoes and error handling. âœ”ï¸

- **Email notifications**
  - Implement e-mail success/failure alerts after each project's run. âœ”ï¸
  - Include ETL run summary, logs and error details.
  - Add business, admin and error recipients. âœ”ï¸
  - Add a boolean operators to control whether the recipients should receive an e-mail. âœ”ï¸
  - Add a `load_smtp_config` function to read e-mail credentials (moved to config_utils.py in utilities). âœ”ï¸ 
  - Add a `add_task_result_to_email` function to build task execution log incrementally. âœ”ï¸ 
  - Add a `add_log_block_to_email` function to build technical log details incrementally. âœ”ï¸
  - Add a `prepare_emails` function to build e-mails based on general info, task execution log and technical log details. âœ”ï¸
  - Add a `send_emails` function that sends the prepared e-mails. âœ”ï¸
  - Add a `smtp_send` function that executes the technical transmission of an email via SMTP. âœ”ï¸

- **Backfill**
  - Implement backfill loading option in `.bat` âœ”ï¸, `_docker.bat` âœ”ï¸, `.sh`, `_docker.sh` files.
  - Create a separate backfill project that accepts `project_name`, `start_date`, `end_date`, `load_days` to run a given project and load data incrementally.

- **Fact and dimensional tables**
  - Add staging and normal fact and dimensional tables (staging.dim â†’ warehouse.dim â†’ staging.fact â†’ warehouse.fact). 
  - Build a staging.dim table to truncate staging area, extract data from the source and add checks for null values, duplicates, missing data.
  - In warehouse.dim, implement slowly changing dimensions - type 1 and type2 updates - close SCD2 rows, insert new SCD2 rows, apply SCD1 updates and insert new rows.
  - Add indices for SCD detection and ETL MERGE/UPDATEs, partial unique index, fast fact and point-in-time lookups, prevent SCD2 ranges overlapping.
  - Implement staging.fact table to truncate staging area, extract data from the source, build the correct fact grain and data quality checks.
  - In warehouse.fact, enforce referential integrity to prevent orphaned surrogate keys, partition the table by date/date_key, add partition-based deletion and build an index strategy.

- **Orchestration**
  - Add an `orchestration` folder with `logs` and `plugins` folders, `.env`, `docker-compose.yaml` and `requirements.txt`
  - Implement orchestration with Airflow and document the steps. âœ”ï¸
  - Add retries, SLA levels, backfilling.
  - Add parametrization for dynamic data handling ({{ ds }})
  - Dependency management - sensors/external task markers and branching.
  - Monitor and maintain with XComs, logs, alerts, and task groups.
  - Build DAGs for specific cases (daily, 30 min , weekly, monthly DAGs).

- **Monitoring**
  - Implement data warehouse health monitoring with dashboards in Apache Superset.
  - Monitor index efficiency and usage, dead tuple counts (vacuum and bloat monitoring).
  - Monitor blocked queries, long-running queries (wait event analysis, query plan regressions, temporary database spills to disk).
  - Monitor buffer cache and I/O performance, connection pool health, add transaction and throughput metrics.
  - Monitor data quality metrics - freshness/latency, volume anomalies, schema evolution, null and uniqueness checks.
  - Monitor cost and resource governance - cost per query, user/role resource consumption, storage growth trends.

- **Data quality checks**
  - Add general data quality checks that run after the DAGs.
  - Monitor record discrepancies (row counts), null values, referential integrity (foreign key checks).
  - Monitor SLA checks (arrival delays), schema drift (new/missing columns), gaps (missing data per day).
  - Late-arriving data handling.

- **Documentation**
  - Add documentation in Confluence for the different processes in the data warehouse.
  - Overall data architecture (data lineage map + entity relationship diagrams).
  - Developer guide: How to build a project.
  - Tech stack (name + version).
  - ETL reference pages.
  - Data governance.

- **Connectors**
  - Add more connectors:
  - Relational databases (OLTP): PostgreSQL âœ”ï¸, MySQL, MSSQL, Oracle
  - Cloud data warehouses (OLAP): Snowflake, Google BigQuery
  - Time-series databases: kdb+
  - Object storage (data lakes): S3 (AWS), Azure Blob Storage, GCS
  - SaaS/API connectors: Salesforce, REST APIs (with requests)
  - SFTP and local files (file based ingestion): SFTP (with pysftp/paramiko), pandas (for local files)

- **Other**
  - Apache Spark - distributed processing system used for big data workloads.
  - Apache Kafka - streaming and event processing.
  - DBT - SQL-based transformation framework that automates the building, testing, and documenting of modular data pipelines.
___
## ğŸ’» Environment setup
- **I. Create a virtual environment (Windows)**
  - Go to your project folder (e.g., cd C:\Users\Mihail\PycharmProjects\datawarehouse)
  - Create the environment â†’ python -m venv venv

- **II. Activate the virtual environment**
  - Activate the environment -> .\venv\Scripts\activate
  - where python â†’ the first path should point to ...\datawarehouse\venv\Scripts\python.exe
- **III. Check locally installed dependencies**
  - pip list

- **IV. Install the dependencies**
  - pip install -r <file_path>\requirements.txt

- **V. Final check**
  - pip list
  - where python â†’ the first path should point to ...\datawarehouse\venv\Scripts\python.exe